h1. Notes From Geeksessions: Beyond the Database
Below are raw notes from tonight's Geeksessions event in San Francisco.  All told, while the speakers did fine, I wasn't wowed with the content.  See for yourself.
h4. Josh Fergus from Sunthe pro-RDBMS guy 
relational dbs good for some things (security), not others (scalability)
data is durable: survives changes to the application
not really much to say, use dbs for what theyre good for
h4. Chad Walters from Powerset on Giant Scale Systems%(=caps)ACID% means constraints 
%(=caps)SQL% is hard to grok at scale
talking about very large computational domains delivered at very high volumes
use a ton of commodity hardware to overcome failures
modern rdbms dont deliver on reliability 
replication starts to be a headache with rdbms
use something like %(=caps)GFS% or Hadoop that was designed for the task
why the application/db divide?
move the computation to the data  MapReduce, specialized data structures
sounds in general like a digestion of the Google approachh4. Paul Querna from BloglinesBloglinesFS: store every blog post ever 
currently storing several billion posts
inspired by MogileFS and %(=caps)GFS%, but different
two main components: PodServer (serves metadata) on ItemDBs (storage nodes)
PodServer: stateless, finds nodes on startup, provides cross-data center replication, spigots for dumping data
ItemDB: serves web sites as chunks, local indexes for for attributes like Date, Post %(=caps)URL%, etc.
request goes from app server to PodServer to ItemDB
crawler writes to PodServer which then writes to ItemDBs
75 data machines of 2?300GB disks, one ItemDB per disk, no %(=caps)RAID%
crawl all blogs every 30 minutes, thousands of concurrent reads + writes per second
wed do it all over again because there arent any choices out there 
Hadoop too specialized for web search, not good for write-heavy data
goal isnt to build an open source project, but to launch a producth4. Arnold Goldberg from eBayeBay by the numbers (not talking about PayPal, Skype, just eBay ecommerce platform)
tons of users, transactions, listings, api developers
600 database instances, 20 billion sql statements per day (95% reads), 5600 active tables, 1.8 petabytes
original pattern: separating writes and reads: write once and replicate to many
replication will kill you if you try to do it at scale [...] its a mess 
new pattern: lookup host finds where your data lives (by primary key)
use persistent database connections, but know the costs: each connection takes a process/thread
figure out where your scalability cliff is and test
connection concentration tier was a pain
to scale, vector based on what youre looking for and distribute
