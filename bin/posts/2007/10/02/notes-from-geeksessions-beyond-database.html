<h1>Notes From Geeksessions: Beyond the Database</h1>
Below are raw notes from tonight's Geeksessions event in San Francisco.  All told, while the speakers did fine, I wasn't wowed with the content.  See for yourself.<br /><br /><h4>Josh Fergus from Sun</h4><ul><li>“the pro-RDBMS guy” </li><br /> <li>relational dbs good for some things (security), not others (scalability)</li><br /> <li>data is “durable”: survives changes to the application</li><br /> <li>not really much to say, use dbs for what they’re good for</li><br /></ul><h4>Chad Walters from Powerset on Giant Scale Systems</h4><ul><li><span class="caps">ACID</span> means constraints </li><br /> <li><span class="caps">SQL</span> is hard to grok at scale</li><br /> <li>talking about very large computational domains delivered at very high volumes</li><br /> <li>use a ton of commodity hardware to overcome failures</li><br /> <li>“modern rdbms don’t deliver on reliability” </li><br /> <li>replication starts to be a headache with rdbms</li><br /> <li>use something like <span class="caps">GFS</span> or Hadoop that was designed for the task</li><br /> <li>why the application/db divide?</li><br /> <li>“move the computation to the data” – MapReduce, specialized data structures</li><br /> <li>sounds in general like a digestion of the Google approach</li></ul><h4>Paul Querna from Bloglines</h4><ul><li>BloglinesFS: “store every blog post ever” </li><br /> <li>currently storing several billion posts</li><br /> <li>inspired by MogileFS and <span class="caps">GFS</span>, but different</li><br /> <li>two main components: PodServer (serves metadata) on ItemDBs (storage nodes)</li><br /> <li>PodServer: stateless, finds nodes on startup, provides cross-data center replication, “spigots” for dumping data</li><br /> <li>ItemDB: serves web sites as “chunks”, local indexes for for attributes like Date, Post <span class="caps">URL</span>, etc.</li><br /> <li>request goes from app server to PodServer to ItemDB</li><br /> <li>crawler writes to PodServer which then writes to ItemDBs</li><br /> <li>75 data machines of 2?300GB disks, one ItemDB per disk, no <span class="caps">RAID</span></li><br /> <li>crawl all blogs every 30 minutes, thousands of concurrent reads + writes per second</li><br /> <li>“we’d do it all over again because there aren’t any choices out there” </li><br /> <li>Hadoop too specialized for web search, not good for write-heavy data</li><br /> <li>goal isn’t to build an open source project, but to launch a product</li></ul><h4>Arnold Goldberg from eBay</h4><ul><li>“eBay by the numbers” (not talking about PayPal, Skype, just eBay ecommerce platform)</li><br /> <li>tons of users, transactions, listings, api developers</li><br /> <li>600 database instances, 20 billion sql statements per day (95% reads), 5600 active tables, 1.8 petabytes</li><br /> <li>original pattern: separating writes and reads: write once and replicate to many</li><br /> <li>“replication will kill you if you try to do it at scale [...] it’s a mess” </li><br /> <li>new pattern: lookup host finds where your data lives (by primary key)</li><br /> <li>use persistent database connections, but know the costs: each connection takes a process/thread</li><br /> <li>figure out where your “scalability cliff” is and test</li><br /> <li>connection concentration tier was a pain</li><br /> <li>to scale, vector based on what you’re looking for and distribute</li></ul>